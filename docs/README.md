# Reddit Financial Sentiment Analyzer

This project scrapes Reddit posts and comments from a specified subreddit, identifies company mentions using Named Entity Recognition (NER), and analyzes the sentiment of each post/comment. The results are saved for further analysis, making it useful for financial research, sentiment tracking, and investment insights.

## Features

- **Reddit Scraping**: Collects posts and comments from a target subreddit using the Reddit API (PRAW).
- **Company Recognition**: Uses Hugging Face NER models to identify company names and tickers in text.
- **Financial Sentiment Analysis**: Uses DistilRoBERTa model fine-tuned on financial news (98.23% accuracy) to classify sentiment with detailed confidence scores.
- **Custom Ticker List**: Matches company mentions against a comprehensive ticker list.
- **Output**: Saves results (including company mentions and sentiment) to JSON and CSV files for further analysis.

## How it Looks

Below are example visualizations generated by the analytics dashboard:

### Mentions Over Time
![Mentions Over Time](../Example/Mentions%20over%20Time.png)

### Positive & Negative Word Clouds
![Positive & Negative Word Clouds](../Example/Positive%20&%20Negative%20Word%20Clouds.png)

### Sample Comments
![Sample Comments](../Example/Sample%20Comments.png)

## Folder Structure

The project is organized into a modular structure to separate concerns and improve maintainability.

```
Sentiment/
├── config/                 # All project configuration
│   └── config.yaml
├── data/                   # Raw and processed data
│   ├── processed/
│   └── raw/
├── docs/                   # Project documentation
│   └── README.md
├── notebooks/              # Jupyter notebooks for exploration
├── requirements.txt        # Project dependencies
├── scripts/                # Standalone and utility scripts
│   ├── check_gpu.py
│   ├── convert_data.py
│   ├── run_scraper.py
│   └── setup_env_vars.sh
├── src/                    # Main application source code
│   ├── api/                # FastAPI backend
│   │   └── main.py
│   ├── core/               # Core components (config, logging)
│   │   ├── config.py
│   │   └── logging_config.py
│   ├── dashboard/          # Streamlit frontend
│   │   └── app.py
│   └── processing/         # Data scraping and NLP
│       └── scraper.py
└── tests/                  # Unit and integration tests
```

## Requirements

- Python 3.10+
- See `requirements.txt` for a full list of dependencies.

You can install all dependencies with a single command:
```bash
pip install -r requirements.txt
```

## Setup

1.  **Clone the Repository**:
    ```bash
    git clone https://github.com/SteelCityAvatar/Sentiment.git
    cd Sentiment
    ```

2.  **Reddit API Credentials**:
    Create a Reddit app at `https://www.reddit.com/prefs/apps` and obtain your credentials.

3.  **Configure Environment Variables**:
    Run the setup script to securely configure your API keys for your WSL environment. It will prompt you for your credentials and add them to your `.bashrc` file.
    ```bash
    ./scripts/setup_env_vars.sh
    ```
    *Note: You may need to make the script executable first with `chmod +x ./scripts/setup_env_vars.sh`.*

4.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

5.  **Review Configuration**:
    Check `config/config.yaml` to ensure the settings (e.g., `subreddit_name`) match your requirements.

## Usage

The project runs as a client-server application, with a data processing pipeline that runs first.

### 1. Run the Data Pipeline
First, run the scraper to gather raw data, and then run the conversion script to prepare it for the API.

-   **Scrape Reddit Data**:
    ```bash
    python scripts/run_scraper.py
    ```
-   **Convert Data to Parquet**:
    ```bash
    python scripts/convert_data.py
    ```

### 2. Launch the Application
The backend API and frontend dashboard must be run in separate terminals.

-   **Terminal 1: Start Backend API**:
    ```bash
    python -m src.api.main
    ```
-   **Terminal 2: Start Frontend Dashboard**:
    ```bash
    streamlit run src/dashboard/app.py
    ```

The dashboard will now be available at `http://localhost:8501`.